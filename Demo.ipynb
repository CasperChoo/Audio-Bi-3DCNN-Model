{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0SbNbKHwYyj",
        "colab_type": "text"
      },
      "source": [
        "## **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Og9qHQhd_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "from google.colab.patches import cv2_imshow\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils\n",
        "%matplotlib inline\n",
        "import os\n",
        "from PIL import Image\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJH4KdRvwfCZ",
        "colab_type": "text"
      },
      "source": [
        "# **Convert label to emotion category**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdbn382VYcYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertLabel(label):\n",
        "    if(label==0):\n",
        "        return \"Anger\"\n",
        "    if(label==1):\n",
        "        return \"Disgust\"\n",
        "    if(label==2):\n",
        "        return \"Fear\"\n",
        "    if(label==3):\n",
        "        return \"Happy\"\n",
        "    if(label==4):\n",
        "        return \"Neutral\"     \n",
        "    if(label==5):\n",
        "        return \"Sad\"\n",
        "    if(label==6):\n",
        "        return \"Surprise\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emzq7a5Owmax",
        "colab_type": "text"
      },
      "source": [
        "# **Paths**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHvDDO2TM3Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input video path\n",
        "videoPath = \"test.avi\"\n",
        "detector_path = \"/cascade/deploy.prototxt.txt\"\n",
        "detector_path2 = \"/cascade/res10_300x300_ssd_iter_140000.caffemodel\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mZGjjXqwqD6",
        "colab_type": "text"
      },
      "source": [
        "## **Extract 5 face and background images from video**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M98-m873Lts1",
        "colab_type": "code",
        "outputId": "8d301f6e-c4fb-49e5-a60a-ad58e474b2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "detector = cv2.dnn.readNetFromCaffe(detector_path, detector_path2)\n",
        "\n",
        "if __name__ == '__main__' :        \n",
        "    cap = cv2.VideoCapture(videoPath);\n",
        "    clip = VideoFileClip(videoPath)\n",
        "            \n",
        "    cap.set(cv2.CAP_PROP_FRAME_COUNT, 400)\n",
        "    length=50\n",
        "    count=0\n",
        "    name_count=1\n",
        "    print(\"Pre-processing Video......\")\n",
        "    while(cap.isOpened()):\n",
        "        count+=1\n",
        "        if length ==count:\n",
        "            break\n",
        "        if name_count==6:\n",
        "            break\n",
        "        ret, img = cap.read()\n",
        "        if img is None:\n",
        "            continue\n",
        "        inputBlob = cv2.dnn.blobFromImage(cv2.resize(img, (224, 224)), 1, (300, 300), (104, 177, 123))\n",
        "        detector.setInput(inputBlob)\n",
        "        detections = detector.forward()\n",
        "        \n",
        "        for i in range(0, detections.shape[2]):\n",
        "            # Probability of prediction\n",
        "            prediction_score = detections[0, 0, i, 2]\n",
        "            if prediction_score < 0.6:\n",
        "                continue\n",
        "            # Finding height and width of frame\n",
        "            (h, w) = img.shape[:2]\n",
        "            # compute the (x, y)-coordinates of the bounding box for the\n",
        "            # object\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (x1, y1, x2, y2) = box.astype(\"int\")\n",
        "            \n",
        "            if y2 > h or x2 > w:\n",
        "                continue\n",
        "            if y2 <= y1 or x2 <= x1:\n",
        "                continue\n",
        "            if y2 ==0 or x2 ==0 or y1 ==0 or x1 ==0:\n",
        "                continue\n",
        "            if y2 -y1<=0 or x2 - x1<=0:\n",
        "                continue\n",
        "            if y1 < 0 or y2<0 or x1<0 or x2<0:\n",
        "                continue\n",
        "\n",
        "            img2 = img[y1:y2, x1:x2]\n",
        "            face_name = 'temp/Face_' +str(name_count)+ '.jpg'               #\n",
        "            cv2.imwrite(face_name, img2)\n",
        "            blank_image = np.zeros(shape=[y2-y1, x2-x1, 3], dtype=np.uint8)\n",
        "            img[y1:y2, x1:x2] = blank_image\n",
        "            background_name = 'temp/Background_' +str(name_count)+ '.jpg'         #\n",
        "            name_count+=1\n",
        "            cv2.imwrite(background_name, img)\n",
        "            \n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    if(name_count!=6):\n",
        "        print(\"Face can't be detected in the video!\")\n",
        "    else:\n",
        "        print(\"Video Pre-processing Completed!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pre-processing Video......\n",
            "Video Pre-processing Completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efwy4AkKhoCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWftiZ0yX3TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1=[]\n",
        "for i in range(1,6):\n",
        "          img_name = 'temp/Face_' +str(i)+ '.jpg'\n",
        "          image = Image.open(str(img_name))\n",
        "          image = data_transforms(image)\n",
        "          X1.append(image.squeeze_(0))\n",
        "X2 = []\n",
        "for i in range(1,6):\n",
        "          img_name = 'temp/Background_' +str(i)+ '.jpg' \n",
        "          image = Image.open(str(img_name))\n",
        "          image = data_transforms(image)\n",
        "          X2.append(image.squeeze_(0))\n",
        "\n",
        "X1 = torch.stack(X1, dim=0).unsqueeze_(0)\n",
        "\n",
        "X2 = torch.stack(X2, dim=0).unsqueeze_(0)\n",
        "\n",
        "x1=X1.squeeze(1)\n",
        "x1=x1.transpose(1,2)\n",
        "x1=x1.type(torch.cuda.FloatTensor)\n",
        "\n",
        "x2=X2.squeeze(1)\n",
        "x2=x2.transpose(1,2)\n",
        "x2=x2.type(torch.cuda.FloatTensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r80OHj84w2kP",
        "colab_type": "text"
      },
      "source": [
        "## **Convert video .mp4 to .wav**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pB3b7vURDbL",
        "colab_type": "code",
        "outputId": "39cf34cc-9c80-4dd2-9e6a-3fac00c5e5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "command=\"ffmpeg -i '{input}' -ac 2 -b:v 2000k -c:a aac -c:v libx264 -b:a 160k -vprofile high -bf 0 -strict experimental -f mp4 '{output}.mp4'\".format(input = videoPath, output = 'temp/output')\n",
        "subprocess.call(command, shell=True)\n",
        "command =\"ffmpeg -i 'output.mp4' -ab 320k -ac 2 -ar 44100 -vn '{}.wav'\".format('temp/output')    \n",
        "subprocess.call(command, shell=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB5og7W4xC57",
        "colab_type": "text"
      },
      "source": [
        "# **Extract audio feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDvle3DBRCWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "songname = 'temp/output.wav'\n",
        "audio_feature=[]\n",
        "y, sr = librosa.load(songname, mono=True, duration=7)\n",
        "chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "rmse = librosa.feature.rmse(y=y)\n",
        "spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "zcr = librosa.feature.zero_crossing_rate(y)\n",
        "mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
        "audio_feature.append(np.mean(chroma_stft))\n",
        "audio_feature.append(np.mean(rmse))\n",
        "audio_feature.append(np.mean(spec_cent))\n",
        "audio_feature.append(np.mean(spec_bw))\n",
        "audio_feature.append(np.mean(rolloff))\n",
        "audio_feature.append(np.mean(zcr))\n",
        "for e in mfcc:\n",
        "    audio_feature.append(np.mean(e))\n",
        "audio = np.array(audio_feature)\n",
        "audio = audio.astype('float').reshape(-1, 26)\n",
        "audio = torch.from_numpy(audio)\n",
        "audio = audio.type(torch.cuda.FloatTensor)\n",
        "audio = audio.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic_lvD4kxLz5",
        "colab_type": "text"
      },
      "source": [
        "# **Build model and load pretrained parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbntyXkBWS97",
        "colab_type": "code",
        "outputId": "af7ae887-ff8e-4744-f010-3d9ebe8338f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "PATH = \"/model/Audio-3D-CNN.pt\"\n",
        "from model.Audio_3D_CNN import Model, Face, Context, Audio\n",
        "model = Model(Face(),Context(),Audio())\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (modelA): Face(\n",
              "    (conv1): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
              "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
              "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool2): MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
              "    (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool3): MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv4): Conv3d(128, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
              "    (bn4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool4): MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv5): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
              "    (bn5): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool5): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)\n",
              "    (fc): Linear(in_features=6400, out_features=7, bias=True)\n",
              "  )\n",
              "  (modelB): Context(\n",
              "    (conv1): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
              "    (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
              "    (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool2): MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv3): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
              "    (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool3): MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv4): Conv3d(128, 256, kernel_size=[1, 3, 3], stride=(1, 1, 1))\n",
              "    (bn4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool4): MaxPool3d(kernel_size=(1, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (conv5): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n",
              "    (bn5): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (pool5): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)\n",
              "    (fc): Linear(in_features=6400, out_features=7, bias=True)\n",
              "  )\n",
              "  (modelC): Audio(\n",
              "    (fc): ModuleList(\n",
              "      (0): Linear(in_features=26, out_features=512, bias=True)\n",
              "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (2): Linear(in_features=256, out_features=7, bias=True)\n",
              "    )\n",
              "    (bn): ModuleList(\n",
              "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): ModuleList(\n",
              "    (0): Linear(in_features=21, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (2): Linear(in_features=256, out_features=7, bias=True)\n",
              "  )\n",
              "  (bn): ModuleList(\n",
              "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU5ZyG_oxT6c",
        "colab_type": "text"
      },
      "source": [
        "# **Predict the emotion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLHN9T8Xd8P",
        "colab_type": "code",
        "outputId": "1a3db558-85a3-41ab-f24c-9e99b54cea05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "output=model(x1,x2,audio)\n",
        "print(convertLabel(np.argmax(output.cpu().detach().numpy())))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0906, -2.9325, -3.1102, -3.1126, -1.6156, -2.6584, -1.3751]],\n",
            "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n",
            "Anger\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
